{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"hWgiQS0zkWJ5"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":1,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-db4c  GCE       3                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":3,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":4,"id":"3259ec85","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /opt/conda/miniconda3/lib/python3.10/site-packages (3.6.3)\n","Requirement already satisfied: click in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk) (1.1.0)\n","Requirement already satisfied: regex in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk) (2022.8.17)\n","Requirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk) (4.66.2)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install nltk"]},{"cell_type":"code","execution_count":5,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from operator import add\n","from nltk.stem import PorterStemmer\n","import builtins\n","\n","import math\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":6,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar 11 14:01 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":7,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":8,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'nettaya-315443382' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name.endswith('.parquet'):\n","        paths.append(full_path+b.name)"]},{"cell_type":"code","execution_count":9,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":10,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import *"]},{"cell_type":"code","execution_count":11,"id":"e4c523e7","metadata":{"id":"b1af29c9","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_body_pairs = parquetFile.select(\"id\", \"text\").rdd\n","inverted_body = InvertedIndex()"]},{"cell_type":"code","execution_count":12,"id":"836780f3","metadata":{},"outputs":[],"source":["porter = PorterStemmer()"]},{"cell_type":"code","execution_count":13,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","\n","def word_count(id, text):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in\n","  `all_stopwords` and return entries that will go into our posting lists.\n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs\n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","    '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [x for x in tokens if x not in all_stopwords]\n","  tokens = [porter.stem(x) for x in tokens]\n","  tuples = []\n","  tf_dict = Counter(tokens)\n","  res = []\n","  [res.append(x) for x in tokens if x not in res]\n","  for t in res:\n","    tuples.append((t,(id,tf_dict[t])))\n","  return tuples\n","\n","def count_doc_len(doc_id, text):\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [x for x in tokens if x not in all_stopwords]\n","  tokens = [porter.stem(x) for x in tokens]\n","  return (doc_id,len(tokens))\n","\n","def tf_for_term_id(doc_id, text):\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [x for x in tokens if x not in all_stopwords]\n","  tuples = []\n","  tokens = [porter.stem(x) for x in tokens]\n","  tf_dict = Counter(tokens)\n","  res = []\n","  [res.append(x) for x in tokens if x not in res]\n","  for t in res:\n","    tuples.append((t,tf_dict[t]))\n","  return tuples\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples\n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n","  return sorted_pl\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  token_df = postings.map(lambda x: (x[0], len(x[1])))\n","  return token_df\n","\n"]},{"cell_type":"code","execution_count":null,"id":"dfaedd33","metadata":{},"outputs":[],"source":["# word_counts_body = doc_body_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","# postings_body = word_counts_body.groupByKey().mapValues(reduce_word_counts)\n","# postings_filtered_body = postings_body.filter(lambda x: len(x[1])>50)"]},{"cell_type":"code","execution_count":null,"id":"9e40cb00","metadata":{},"outputs":[],"source":["len_docs_body = doc_body_pairs.map(lambda x: count_doc_len(x[0], x[1]))\n","len_docs_body = len_docs_body.collectAsMap()\n","inverted_body.DL = len_docs_body"]},{"cell_type":"code","execution_count":null,"id":"7ab13cf3","metadata":{},"outputs":[],"source":["# write the global stats out\n","inverted_body.write_index('.', 'index_body_DL')\n","# !ls -l index_body.pkl\n","# upload to gs\n","index_src = \"index_body_DL.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp_body_DL/{index_src}'\n","!gsutil cp index_body_DL.pkl gs://nettadl-315443382/postings_gcp_body_DL/index_body_DL.pkl"]},{"cell_type":"code","execution_count":null,"id":"b8c59d02","metadata":{},"outputs":[],"source":["NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def partition_postings_and_write(postings,base_dir):\n","  ''' A function that partitions the posting lists into buckets, writes out\n","  all posting lists in a bucket to disk, and returns the posting locations for\n","  each bucket. Partitioning should be done through the use of `token2bucket`\n","  above. Writing to disk should use the function  `write_a_posting_list`, a\n","  static method implemented in inverted_index_colab.py under the InvertedIndex\n","  class.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and\n","      offsets its posting list was written to. See `write_a_posting_list` for\n","      more details.\n","  '''\n","  bucket_name = \"nettaya-315443382\"\n","\n"," # Assume postings is an RDD or a similar collection where each item is a (word, posting list) pair\n"," # First, transform postings into (bucket_id, (word, posting list)) for grouping using your `token2bucket_id` logic\n","  postings_by_bucket = postings.map(lambda x: (token2bucket_id(x[0]), x))\n","\n","  # Group postings by bucket_id and prepare them for writing\n","  postings_grouped = postings_by_bucket.groupByKey().mapValues(list)\n","\n","  # For each bucket, write postings to disk and collect location info\n","  postings_locations = postings_grouped.map(lambda x: InvertedIndex.write_a_posting_list(x, base_dir, bucket_name))\n","\n","  return postings_locations"]},{"cell_type":"code","execution_count":null,"id":"14228381","metadata":{},"outputs":[],"source":["postings_locs_body = partition_postings_and_write(postings_filtered_body,'text').collect()"]},{"cell_type":"code","execution_count":14,"id":"ab3296f4","metadata":{"id":"Opl6eRNLM5Xv","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs_body = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='text'):\n","    if not blob.name.endswith(\".pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs_body[k].extend(v)"]},{"cell_type":"code","execution_count":15,"id":"a5d2cfb6","metadata":{"id":"54vqT_0WNc3w"},"outputs":[],"source":["inverted_body.posting_locs = super_posting_locs_body"]},{"cell_type":"code","execution_count":16,"id":"865e7a3e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://index_body_posting_locs.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][ 10.6 MiB/ 10.6 MiB]                                                \n","Operation completed over 1 objects/10.6 MiB.                                     \n"]}],"source":["# write the global stats out\n","inverted_body.write_index('.', 'index_body_posting_locs')\n","# !ls -l index_body.pkl\n","# upload to gs\n","index_src = \"index_body_posting_locs.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp_body_posting_locs/{index_src}'\n","!gsutil cp index_body_posting_locs.pkl gs://nettaya-315443382/postings_gcp_body_posting_locs/index_body_posting_locs.pkl"]},{"cell_type":"code","execution_count":null,"id":"27b1a309","metadata":{},"outputs":[],"source":["# Add the token - df dictionary to the inverted index\n","# inverted_body.df = w2df_body_dict\n","# Adding the posting locations dictionary to the inverted index"]},{"cell_type":"code","execution_count":null,"id":"12f0ede2","metadata":{},"outputs":[],"source":["# def calculate_tfidf_doc(tf_term_doc, index):\n","#     DL = index.DL\n","#     N = len(DL)\n","#     df_dict = index.df\n","#     doc_term_tfidf = tf_term_doc.map(lambda x : (x[0] , math.sqrt(builtins.sum([math.pow((tf/DL[x[0]]) * (math.log(N/df_dict[term],10)),2) for term, tf in x[1] if term in df_dict and x[0] in index.DL]))))\n","#     return doc_term_tfidf"]},{"cell_type":"code","execution_count":null,"id":"2711a563","metadata":{},"outputs":[],"source":["# tf_term_doc_body = doc_text_pairs.map(lambda x : (x[0], tf_for_term_id(x[0], x[1])))\n","# tfidf_body = calculate_tfidf_doc(tf_term_doc_body, inverted_body)\n","# tfidf_dict_body = tfidf_body.collectAsMap()"]},{"cell_type":"code","execution_count":null,"id":"14fd9074","metadata":{},"outputs":[],"source":["# total_terms_body = postings_filtered_body.flatMapValues(lambda x : x).map(lambda x: (x[0],x[1][1])).reduceByKey(add)\n","# inverted_body.term_total = total_terms_body.collectAsMap()"]},{"cell_type":"code","execution_count":null,"id":"9e8a1c47","metadata":{},"outputs":[],"source":["# w2df_body = calculate_df(postings_filtered_body)\n","# w2df_body_dict = w2df_body.collectAsMap()"]},{"cell_type":"code","execution_count":null,"id":"49f9ab0b","metadata":{},"outputs":[],"source":["# write_pickle(inverted_body.DL, \"body_DL\",bucket_name,\"body\" )"]},{"cell_type":"code","execution_count":null,"id":"0c2ae13e","metadata":{},"outputs":[],"source":["# write_pickle(inverted_body.term_total, \"body_term_total\",bucket_name,\"body\")"]},{"cell_type":"code","execution_count":null,"id":"b787f07b","metadata":{},"outputs":[],"source":["# write_pickle(inverted_body.vec_len_doc, \"vec_len_total\",bucket_name,\"body\")"]},{"cell_type":"code","execution_count":null,"id":"3ee2e120","metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}